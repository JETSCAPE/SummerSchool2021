{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For wider display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 1 ############## <-- Please refer this block number when you ask questions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(\"notebook\")\n",
    "\n",
    "# A useful plotting function that displays joint distribution of x and y, \n",
    "# and marginalized distribuitions of each.\n",
    "# Not important\n",
    "def hist_1d_2d(X, Y, nameX, nameY):\n",
    "    left, width = 0.1, 0.75\n",
    "    bottom, height = 0.1, 0.75\n",
    "    spacing = 0.005\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom + height + spacing, width, 0.15]\n",
    "    rect_histy = [left + width + spacing, bottom, 0.15, height]\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_axes(rect_scatter)\n",
    "    ax1 = fig.add_axes(rect_histx, sharex=ax)\n",
    "    ax2 = fig.add_axes(rect_histy, sharey=ax)\n",
    "    ax1.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax2.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    ax.scatter(X, Y)\n",
    "    ax1.hist(X, density=True)\n",
    "    ax2.hist(Y, orientation='horizontal', density=True)\n",
    "    ax.set_xlabel(nameX)\n",
    "    ax.set_ylabel(nameY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Example (30 mins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process (unconditioned)\n",
    "\n",
    "An uncondititoned Gaussian Process can be viewed as a random function $G(x)$, specified by the mean (one-point function)\n",
    "$$\\langle G(x)\\rangle \\equiv \\mu(x),$$ \n",
    "and covariance function (two-point function)\n",
    "$$\\langle\\delta G(x) \\delta G(x')\\rangle \\equiv k(x, x').$$\n",
    "And we assume any combinations of the random variable $\\{ G(x_1), G(x_2), \\cdots, G(x_n)\\}$ forms an $n$-dimensional Normal distribution with the above mean and pairwise covaraince.\n",
    "\n",
    "**Let's examine a conceret example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single and multi-variated Gaussian distribution\n",
    "\n",
    "1. A single-variable Gaussian distribution is charactered by a mean $\\mu$ and a standard deviation $\\sigma$\n",
    "\n",
    "$$f(x; \\mu, \\sigma) = \\frac{1}{(2\\pi)^{1/2}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "2. A multi-varaite Gaussian distribution is charactered by the means $\\mu(x_1), \\mu(x_2),\\cdots$ of each variable and the covariance matrix:\n",
    "$$ \\Sigma = \\begin{bmatrix} s_{11} & s_{12} & \\cdots\\\\\n",
    "s_{21} & s_{22} & \\cdots\\\\ \n",
    "\\cdots & \\cdots & \\cdots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "with $\\sigma_{12}^2 <\\sigma_{11} \\sigma_{22}$\n",
    "\n",
    "\n",
    "$$f(x_1, x_2; \\mu_1, \\mu_2, \\Sigma_{ij}) = \\frac{1}{(2\\pi)^{1/2}\\sqrt{|\\Sigma|}} \\exp\\left\\{-\\frac{1}{2}\\sum_{i,j=1}^2(x_i-\\mu_i)^T \\Sigma_{ij}^{-1} (x_j-\\mu_j)\\right\\} $$\n",
    "\n",
    "We will take a specific form of the kernel function \n",
    "$$k(x_1, x_2) = C^2 \\exp\\left\\{-\\frac{|x_1-x_2|^2}{2L^2}\\right\\}$$\n",
    "**Output values of close input points $x_{1,2}$ have strongly correlated / close outputs $y_{1,2}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 2 ##############\n",
    "\n",
    "# The Gaussian-like two-point function for filling the elements of the covariance matrix\n",
    "# cov(x1, x2) = C^2 exp(-(x1-x2)^2/(2L^2) )\n",
    "# C: std of the distriubtion at one point\n",
    "# L: correlation length scale\n",
    "@np.vectorize\n",
    "def kernel(x1, x2, C, L):\n",
    "    return C**2 * np.exp(-.5*(x1-x2)**2/L**2)\n",
    "\n",
    "# visualize a 10 by 10 covariance matrix:\n",
    "N = 11\n",
    "x = np.linspace(-1,1,N)\n",
    "mean = np.zeros_like(x)\n",
    "cov = kernel(*np.meshgrid(x,x), C=1, L=1)\n",
    "plt.imshow(cov, extent=[-1,1,1,-1], vmin=0, vmax=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 3 ##############\n",
    "# Function that samples N-D normal distribution \n",
    "# with x = [-2.0, ... N points ... 2.0]\n",
    "#      mean = [0, ..., 0]\n",
    "#      C_ij = Cov(x_i, x_j) = kernel(x_i, x_j; C, L)\n",
    "# generate 1000 lines by default\n",
    "def ND_Normal(N, C, L, nsamples=1000):\n",
    "    x = np.linspace(-2,2,N)\n",
    "    mean = np.zeros_like(x)\n",
    "    cov = kernel(*np.meshgrid(x,x), C, L)\n",
    "    return x, np.random.multivariate_normal(mean, cov, nsamples)\n",
    "\n",
    "# Generate an ensemble of size-20 random vectors from a distribution with C=1 and L=1\n",
    "x, Y = ND_Normal(N=20, C=1, L=1)\n",
    "\n",
    "# Checkout the joint distribution of the ith and jth random varible\n",
    "i, j = 1,2\n",
    "hist_1d_2d(Y[:,i], Y[:,j], '$x_{}$'.format(i), '$x_{}$'.format(j))\n",
    "plt.title(r\"$|x_i-x_j|/L = {:1.2f}$\".format(np.abs(x[i]-x[j])/1))\n",
    "\n",
    "# <How does it change when you change the separation of x_i and x_j?>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at this multi-variate normal, plot $x$ and realizations $y(x)$ as a scattered plot. Since close inputs gaurantees close outputs, the resulting plots are samples of smooth functions with the given variance and correlation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 4 ##############\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(8,4))\n",
    "_ = ax1.plot(x, Y[:5].T, 'ro-')\n",
    "ax1.set_title(\"5 realizations of the random function\")\n",
    "\n",
    "CLbins = [0,60,90,95]\n",
    "ax2.set_title(\"Probability distribution of values of $G(x)$\")\n",
    "for CL, opacity in zip(CLbins, [1., .4, .3, .2, .1]):\n",
    "    lower, upper = np.percentile(Y, [50-CL/2., 50+CL/2.], axis=0)\n",
    "    ax2.fill_between(x, lower, upper, color='r', alpha=opacity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP-1: Conditioning the GP\n",
    "Now, we would like to pick a particular subset of random functions that comes near to the points\n",
    "$$(x^*_i, y^*_i) =  (-2,-1), (0,0.5),(2,0.7)$$.\n",
    "To do this, we picks random functions that statisfies $$|G(x^*_i)-y^*_i | < \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 5 ##############\n",
    "\n",
    "# generate a large ensemble with correlation length 2\n",
    "x, Y = ND_Normal(N=21, C=1, L=2, nsamples=100000)\n",
    "\n",
    "# a filter \"cut\" to select those random functions that approaches (within +/- 0.1)\n",
    "# the desired points (-2,1), (0, 0.5), and (2, 0.7)\n",
    "epsilon = 0.1\n",
    "cut = ( np.abs(Y[:,0]+1)<epsilon ) \\\n",
    "    & ( np.abs(Y[:,10]-.5)<epsilon ) \\\n",
    "    & ( np.abs(Y[:,20]-.7)<epsilon ) \\\n",
    "\n",
    "# Now plot the \n",
    "fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "CLbins = [0,60,90,95]\n",
    "for CL, opacity in zip(CLbins, [1., .4, .3, .2, .1]):\n",
    "    lower, upper = np.percentile(Y, [50-CL/2., 50+CL/2.], axis=0)\n",
    "    ax.fill_between(x, lower, upper, color='r', alpha=opacity)\n",
    "ax.errorbar([-2,0,2],[-1,.5,.7],yerr=epsilon,fmt='kD')\n",
    "_ = ax.plot(x, Y[cut].T,'b-')\n",
    "_ = ax.plot(x, Y[:5].T,'r-')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "# Questions: \n",
    "# What if you use a L that is very small compared to the \n",
    "# typical lengh-scale of variaton of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditioned set of random functions provides an interpolation of the three conditioned points with their spread as uncertainties. The natural inclusion of interpolation uncertainty is a big advantage of GP in the workflow of Bayesian analysis of complex model.\n",
    "\n",
    "### The variance and the correlations length $C^2, L$.\n",
    "Strickly spearking, the variance and the correlation length is unknown for a given set of data to be interpolated. An common practice is to optimize the values of $C$ and $L$ in the \"fitting\" so that it maximumize the likelihood of desribing the data. The systematic tuning for an optimal set of $C$ and $L$ is the so-called training process.\n",
    "\n",
    "For practical use, we can use well developed GP modules in sklearn. It implements different kinds of kernel functions and training algorithms.\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/gaussian_process.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying sklearn GP to 1D inference/interpolation\n",
    "\n",
    "**The problem**: given values of function $F(x)$ on a sparse grid $x_i, i=1,2,\\cdots$, use Gaussain process emulator (regressor) to infer the functional form of the $F$.\n",
    "\n",
    "$$F(x) = x^2 + \\sin(5x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 6 ##############\n",
    "# The function to be emulated / interpolated\n",
    "def F(x):\n",
    "    return x**2+np.sin(5*x)\n",
    "\n",
    "# We only take six \"measurements\" of this \"unknown function\" within [-1, 1]\n",
    "# In this example, we consider these measurements are accurate\n",
    "x_design = np.linspace(-1,1,6)\n",
    "y_design = F(x_design)\n",
    "plt.plot(x_design, y_design, 'ro', label='Design')\n",
    "plt.legend()\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 7 ##############\n",
    "from sklearn.gaussian_process import \\\n",
    "     GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process import kernels\n",
    "\n",
    "# Train the emulator, specify the kernel functions \n",
    "kernel = \\\n",
    "    1. * kernels.RBF(length_scale=1., # <-- Initial guesses of C and L: C=1, L=1\n",
    "                     length_scale_bounds=(.1,10) # <-- we will let the optimzier to find an optimal value of \n",
    "                                                 # L within 0.1 to 10.0\n",
    "    )\n",
    "\n",
    "# initialize the guassian process, and allow it to optimzie 5 times, incase it falls in local minima\n",
    "gp = GPR(kernel=kernel, n_restarts_optimizer=5)\n",
    "# Train it on the design points\n",
    "gp.fit(np.atleast_2d(x_design).T, y_design)\n",
    "# Print out the optimzied C and L\n",
    "print(\"C^2 = \", gp.kernel_.get_params()['k1'])\n",
    "print(\"L = \", gp.kernel_.get_params()['k2'])\n",
    "\n",
    "# Score for describing the training data\n",
    "print(\"This score of describing the training data:\", gp.score(np.atleast_2d(x_design).T, y_design))\n",
    "# Score for describing a another set of data (validation points), \n",
    "# this is important to make sure we are not overfitting!\n",
    "xv = np.linspace(-1,1,31)\n",
    "print(\"This score of describing validation data:\", gp.score(np.atleast_2d(xv).T, F(xv)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 8 ##############\n",
    "# A wrapper to make predictions from GP, and also returns the standard deviation of the ensmeble \n",
    "# of conditioned random functions as an interpolation uncertainty.\n",
    "# Uncertainty estimation is important for applying Bayes theorem.\n",
    "# You will find in many cases with high-dimensional parameter space that \n",
    "# interpolation uncertainties dominates the experimetnal one and maybe even theoretical one\n",
    "def predict(x, gp):\n",
    "    mean, cov = gp.predict(return_cov=True, X=np.atleast_2d(x).T)\n",
    "    return mean, np.sqrt(np.diag(cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 9 ##############\n",
    "# compare the mean, +/-1 and 2 sigma bands of the GP prediction to the truth\n",
    "\n",
    "x = np.linspace(-1,1,101)\n",
    "y, ystd = predict(x, gp)\n",
    "plt.plot(x_design, y_design, 'ro', label='Design')\n",
    "plt.plot(x, F(x),'k-', label=r'$F(x)$')\n",
    "plt.plot(x, y,'b--', label=r'GP mean')\n",
    "plt.fill_between(x, y-ystd, y+ystd, color='b', alpha=.3, label=r'GP $\\pm 1\\sigma$')\n",
    "plt.fill_between(x, y-2*ystd, y+2*ystd, color='gray', alpha=.3, label=r'GP $\\pm 2\\sigma$')\n",
    "plt.xlabel(r\"Input $x$\")\n",
    "plt.ylabel(r\"Output $y=F(x)$\")\n",
    "plt.legend()\n",
    "\n",
    "# Question: \n",
    "# 1. Can you still get a sensible interpolation if you choose less design points?\n",
    "# 2. Does this Gaussian Process provide good extrapolation? What happens when you make predictions outside [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
