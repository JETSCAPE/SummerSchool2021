{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 1 ############## <-- Please refer this block number when you ask questions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(\"notebook\")\n",
    "\n",
    "# a useful plotting function\n",
    "def hist_1d_2d(X, Y, nameX, nameY):\n",
    "    left, width = 0.1, 0.75\n",
    "    bottom, height = 0.1, 0.75\n",
    "    spacing = 0.005\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom + height + spacing, width, 0.15]\n",
    "    rect_histy = [left + width + spacing, bottom, 0.15, height]\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_axes(rect_scatter)\n",
    "    ax1 = fig.add_axes(rect_histx, sharex=ax)\n",
    "    ax2 = fig.add_axes(rect_histy, sharey=ax)\n",
    "    ax1.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax2.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    ax.scatter(X, Y)\n",
    "    ax1.hist(X, density=True)\n",
    "    ax2.hist(Y, orientation='horizontal', density=True)\n",
    "    ax.set_xlabel(nameX)\n",
    "    ax.set_ylabel(nameY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Example (30 mins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process (unconditioned)\n",
    "\n",
    "An uncondititoned Gaussian Process can be viewed as a random function $G(x)$, specified by the mean (one-point function)\n",
    "$$\\langle G(x)\\rangle \\equiv \\mu(x),$$ \n",
    "and covariance function (two-point function)\n",
    "$$\\langle\\delta G(x) \\delta G(x')\\rangle \\equiv k(x, x').$$\n",
    "And we assume any combinations of the random variable $\\{ G(x_1), G(x_2), \\cdots, G(x_n)\\}$ forms an $n$-dimensional Normal distribution with the above mean and pairwise covaraince.\n",
    "\n",
    "**Let's examine a conceret example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single and multi-variated Gaussian distribution\n",
    "\n",
    "1. A single-variable Gaussian distribution is charactered by a mean $\\mu$ and a standard deviation $\\sigma$\n",
    "\n",
    "$$f(x; \\mu, \\sigma) = \\frac{1}{(2\\pi)^{1/2}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} $$\n",
    "\n",
    "2. A multi-varaite Gaussian distribution is charactered by the means $\\mu(x_1), \\mu(x_2),\\cdots$ of each variable and the covariance matrix:\n",
    "$$ \\Sigma = \\begin{bmatrix} s_{11} & s_{12} & \\cdots\\\\\n",
    "s_{21} & s_{22} & \\cdots\\\\ \n",
    "\\cdots & \\cdots & \\cdots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "with $\\sigma_{12}^2 <\\sigma_{11} \\sigma_{22}$\n",
    "\n",
    "\n",
    "$$f(x_1, x_2; \\mu_1, \\mu_2, \\Sigma_{ij}) = \\frac{1}{(2\\pi)^{1/2}\\sqrt{|\\Sigma|}} \\exp\\left\\{-\\frac{1}{2}\\sum_{i,j=1}^2(x_i-\\mu_i)^T \\Sigma_{ij}^{-1} (x_j-\\mu_j)\\right\\} $$\n",
    "\n",
    "We will take a specific form of the kernel function \n",
    "$$k(x_1, x_2) = C^2 \\exp\\left\\{-\\frac{|x_1-x_2|^2}{2L^2}\\right\\}$$\n",
    "**Output values of close input points $x_{1,2}$ have strongly correlated / close outputs $y_{1,2}$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 2 ##############\n",
    "@np.vectorize\n",
    "def kernel(x1, x2, C, L):\n",
    "    return C**2 * np.exp(-.5*(x1-x2)**2/L**2)\n",
    "def ND_Normal(N=2, C=1, L=1, nsamples=1000):\n",
    "    x = np.linspace(-1,1,N)\n",
    "    mean = np.zeros_like(x)\n",
    "    cov = kernel(*np.meshgrid(x,x), C, L)\n",
    "    return x, np.random.multivariate_normal(mean, cov, nsamples)\n",
    "\n",
    "x, Y = ND_Normal(N=10, C=1, L=1)\n",
    "hist_1d_2d(Y[:,0], Y[:,1], '$x_1$', '$x_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to look at this multi-variate normal, plot $x$ and realizations $y$ as a scattered plot. Since close inputs gaurantees close outputs, the resulting plots are samples of smooth functions with the given variance and correlation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 3 ##############\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(8,4))\n",
    "_ = ax1.plot(x, Y[:20].T, 'ro-')\n",
    "ax1.set_title(\"20 realizations of the rasndom function\")\n",
    "\n",
    "CLbins = [0,60,90,95]\n",
    "ax1.set_title(\"Probability distribution of values of $G(x)$\")\n",
    "for CL, opacity in zip(CLbins, [1., .4, .3, .2, .1]):\n",
    "    lower, upper = np.percentile(Y, [50-CL/2., 50+CL/2.], axis=0)\n",
    "    ax2.fill_between(x, lower, upper, color='r', alpha=opacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GP-1: Conditioning the GP\n",
    "Now, we would like to pick a particular subset of random functions that comes near to the points\n",
    "$$(x^*_i, y^*_i) =  (-1,-1), (0,0.5),(1,0.7)$$.\n",
    "To do this, we picks random functions that statisfies $$|G(x^*_i)-y^*_i | < \\epsilon$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 4 ##############\n",
    "x, Y = ND_Normal(N=11, C=1, L=1, nsamples=100000)\n",
    "epsilon = 0.1\n",
    "cut = ( np.abs(Y[:,0]+1)<epsilon ) \\\n",
    "    & ( np.abs(Y[:,5]-.5)<epsilon ) \\\n",
    "    & ( np.abs(Y[:,10]-.7)<epsilon ) \\\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "CLbins = [0,60,90,95]\n",
    "ax.set_title(\"Probability distribution of values of $G(x)$\")\n",
    "for CL, opacity in zip(CLbins, [1., .4, .3, .2, .1]):\n",
    "    lower, upper = np.percentile(Y, [50-CL/2., 50+CL/2.], axis=0)\n",
    "    ax.fill_between(x, lower, upper, color='r', alpha=opacity)\n",
    "ax.errorbar([-1,0,1],[-1,.5,.7],yerr=epsilon,fmt='kD')\n",
    "_ = ax.plot(x, Y[cut].T,'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditioned set of random functions provides an interpolation of the three conditioned points with their spread as uncertainties. The natural inclusion of interpolation uncertainty is a big advantage of GP in the workflow of Bayesian analysis of complex model.\n",
    "\n",
    "### The variance and the correlations length $C^2, L$.\n",
    "Strickly spearking, the variance and the correlation length is unknown for a given set of data to be interpolated. An common practice is to optimize the values of $C$ and $L$ in the \"fitting\" so that it maximumize the likelihood of desribing the data. The systematic tuning for an optimal set of $C$ and $L$ is the so-called training process.\n",
    "\n",
    "For practical use, we can use well developed GP modules in sklearn. It implements different kinds of kernel functions and training algorithms.\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/gaussian_process.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying sklearn GP to 1D inference/interpolation\n",
    "\n",
    "**The problem**: given values of function $F(x)$ on a sparse grid $x_i, i=1,2,\\cdots$, use Gaussain process emulator (regressor) to infer the functional form of the $F$.\n",
    "\n",
    "$$F(x) = x^2 + \\sin(5x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 5 ##############\n",
    "# This is function to be emualted\n",
    "def F(x):\n",
    "    return x**2+np.sin(5*x)\n",
    "\n",
    "x_design = np.linspace(-1,1,6)\n",
    "y_design = F(x_design)\n",
    "plt.plot(x_design, y_design, 'ro', label='Design')\n",
    "x = np.linspace(-1,1,101)\n",
    "plt.plot(x, F(x),'k-', label=r'$F(x)$')\n",
    "plt.xlabel(r\"$x$\")\n",
    "plt.ylabel(r\"$y$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 6 ##############\n",
    "from sklearn.gaussian_process import \\\n",
    "     GaussianProcessRegressor as GPR\n",
    "from sklearn.gaussian_process import kernels\n",
    "\n",
    "# Train the emulator, specify the kernel functions \n",
    "kernel = \\\n",
    "    1 * kernels.RBF(\n",
    "        length_scale=1.,\n",
    "        length_scale_bounds=(.1,10)\n",
    "    )\n",
    "\n",
    "gp = GPR(kernel=kernel, n_restarts_optimizer=5)\n",
    "gp.fit(np.atleast_2d(x_design).T, y_design)\n",
    "print(\"C^2 = \", gp.kernel_.get_params()['k1'])\n",
    "print(gp.kernel_.get_params()['k2'])\n",
    "print(\"This score of describing the training data:\", gp.score(np.atleast_2d(x_design).T, y_design))\n",
    "\n",
    "xv = np.linspace(-1,1,11)\n",
    "print(\"This score of describing validation data:\", gp.score(np.atleast_2d(xx).T, F(xv)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 7 ##############\n",
    "# A wrapper to make predictions from GP\n",
    "def predict(x, gp):\n",
    "    mean, cov = gp.predict(return_cov=True, X=np.atleast_2d(x).T)\n",
    "    return mean, np.sqrt(np.diag(cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Block 8 ##############\n",
    "x = np.linspace(-1,1,101)\n",
    "y, ystd = predict(x, gp)\n",
    "\n",
    "plt.plot(x_design, y_design, 'ro', label='Design')\n",
    "plt.plot(x, F(x),'k-', label=r'$F(x)$')\n",
    "plt.plot(x, y,'b--', label=r'GP mean')\n",
    "plt.fill_between(x, y-ystd, y+ystd, color='b', alpha=.3, label=r'GP $\\pm 1\\sigma$')\n",
    "plt.fill_between(x, y-2*ystd, y+2*ystd, color='gray', alpha=.3, label=r'GP $\\pm 2\\sigma$')\n",
    "plt.xlabel(r\"Input $x$\")\n",
    "plt.ylabel(r\"Output $y=F(x)$\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Go back to make the prediction range of $x$ larger, `[-2,2]` for instance. Does this Gaussian provide good extrapolation? (However, it is possible to use GP to make projections)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
