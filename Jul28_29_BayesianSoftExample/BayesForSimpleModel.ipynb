{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For wider display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 1 #### Please refer to this number in your questions\n",
    "import subprocess\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os, pickle, math\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(\"notebook\")\n",
    "sns.color_palette('bright')\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as gpr\n",
    "from sklearn.gaussian_process import kernels as krnl\n",
    "\n",
    "import scipy.stats as st\n",
    "from scipy import optimize\n",
    "from scipy.linalg import lapack\n",
    "\n",
    "from pyDOE import lhs\n",
    "import emcee\n",
    "import h5py\n",
    "\n",
    "# You can find the results, plots and saved d\n",
    "project = \"SimpleBulk\"\n",
    "FIGURE_ID = project + '/plots' # save all figures\n",
    "DATA_ID = project + \"/Data\" # save design points (model calculations + design parameter)\n",
    "for it in [project, FIGURE_ID, DATA_ID]:\n",
    "    os.makedirs(it, exist_ok=True)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(os.path.join(FIGURE_ID, fig_id) + \".png\", format='png', dpi=300)\n",
    "    \n",
    "def hist_1d_2d(X, Y, nameX, nameY):\n",
    "    left, width = 0.1, 0.75\n",
    "    bottom, height = 0.1, 0.75\n",
    "    spacing = 0.005\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom + height + spacing, width, 0.15]\n",
    "    rect_histy = [left + width + spacing, bottom, 0.15, height]\n",
    "\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    ax = fig.add_axes(rect_scatter)\n",
    "    ax1 = fig.add_axes(rect_histx, sharex=ax)\n",
    "    ax2 = fig.add_axes(rect_histy, sharey=ax)\n",
    "    ax1.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax2.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    ax.scatter(X, Y)\n",
    "    ax1.hist(X, density=True)\n",
    "    ax2.hist(Y, orientation='horizontal', density=True)\n",
    "    ax.set_xlabel(nameX)\n",
    "    ax.set_ylabel(nameY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Parameter Inference: a *simple* model for bulk physics.\n",
    "\n",
    "### Bulk models:\n",
    "1. Initial condition: initial condition energy density, geometry, $e(x_\\perp)$\n",
    "2. Dynamical models (JETSCAPE):\n",
    "   * Preequilbirum evolution\n",
    "   * Hydrodynamic evolution (take $\\eta/s(T), \\zeta/s(T)$ as input)\n",
    "   * Particlziation + hadronic scatterings / decays\n",
    "3. Bulk observables: dynamical evolution translates initial condition into final-state observables such as\n",
    "   * particle multiplicity\n",
    "   * anisotropy flows and correlations\n",
    "   * ...\n",
    "\n",
    "### Target quantity $\\eta/s(T)$: \n",
    "We want extract the temperature-dependent shear viscosity. It is parameterized as (same functional form as in recent JETSCAPE paper https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.126.242301 )\n",
    "$$\\frac{\\eta}{s}(T) = a + \n",
    "\\begin{cases} \n",
    "b (T - d), & T>d\\\\\n",
    "c (d - T), & T<d\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### A simple dynamic model used in this notebook:\n",
    "We consider hydrodynamic-like response to the centrality averaged initial condition quantities: unnormalized multiplicity $\\sim d\\tilde{E}_T/d\\eta_s$ ($\\tilde{E}_T$ for short), unnormalized avaraged initial temperature $\\langle \\tilde{T}_0 \\rangle$, and initial geoemtric eccentricity $\\epsilon_2$. We assume they are mapped to the final-state charged particle multiplictiy and elliptic flow via an \"evolution history averaged\" viscosity (https://arxiv.org/abs/1912.06287) $$(\\eta/s)_{\\rm eff} = \\frac{\\int_{T_1}^{T_0} T^{p} \\left(\\eta/s\\right)_T dT }{\\int_{T_1}^{T_0} T^{p} dT } $$ ($p$ controls the sensitvitiy to high-$T$ values of $\\eta/s$, let's take $p=-1$ for now)\n",
    "and \n",
    "$$\n",
    "N_{\\rm ch} = {\\rm Norm} \\cdot \\tilde{E}_T  \\left[1 + (\\eta/s)_{\\rm eff}\\right], \\quad \\quad v_2 = \\epsilon_2 \\cdot \\exp\\left[-50\\frac{(\\eta/s)_{\\rm eff}}{N_{\\rm ch}^{1/3}}\\right]\n",
    "$$\n",
    "Parameters: $\\{{\\rm Norm}, a, b, c, d\\}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tpower = -1\n",
    "Tmin = 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 2 #### Please refer to this number in your questions\n",
    "cen, _, _, ET, ed, e2 = np.loadtxt(DATA_ID+\"/Geometry_info.dat\").T\n",
    "fig, axes = plt.subplots(1,3, figsize=(10,3.5), sharex=True)\n",
    "titles = [r\"Unnormed $N_{\\rm ch}$\", r\"Unnormed $T^3(\\tau_0)$\", r\"$e_2$\"]\n",
    "for ax,y,name in zip(axes, [ET, ed, e2], titles):\n",
    "    ax.plot(cen, y, 'kD')\n",
    "    ax.set_title(name)\n",
    "    ax.set_xlabel(\"Centrality (%)\")\n",
    "    ax.set_ylim(ymin=0)\n",
    "plt.tight_layout()\n",
    "save_fig(\"Initial_condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 3 #### Please refer to this number in your questions\n",
    "# Toy model of a hydro-like repsonse to eccentricity\n",
    "from scipy.integrate import quad\n",
    "def etas(T, params):\n",
    "    etasA, etasB, etasC, etasD = params\n",
    "    res = etasA + etasB*(T-etasD)*(T>etasD) + etasC*(etasD-T)*(T<etasD)\n",
    "    if not hasattr(T, \"__len__\"):\n",
    "        return np.max([res, 0.])\n",
    "    else:\n",
    "        res[res<0.] = 0.\n",
    "        return res\n",
    "\n",
    "def etas_eff(Tmax, params):\n",
    "    N = quad(lambda T: T**Tpower * etas(T, params), Tmin, Tmax)[0]\n",
    "    D = quad(lambda T: T**Tpower, Tmin, Tmax)[0]\n",
    "    return N/D\n",
    "\n",
    "def ToyModel(params):\n",
    "    norm = params[0]\n",
    "    Tmax_array = (norm*ed)**(1./3.)\n",
    "    etas_avg = np.array([etas_eff(Tmax, params[1:]) for Tmax in Tmax_array])\n",
    "    mult = norm * ET * (1. + etas_avg)\n",
    "    v2 = e2 * np.exp(- 50 * etas_avg / (mult)**.3333 ) \n",
    "    return mult, v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Make Design\n",
    "\n",
    "We will sample 200 design points in the 5-dimensional parameter space (50/Dimension). Set the `read_design_from_file` to decide whether read exisiting design points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load design from file. Design shape is (200, 5)\n"
     ]
    }
   ],
   "source": [
    "#### Block 4 #### Please refer to this number in your questions\n",
    "ParameterLabels = ['$N$', '$\\eta/s(A)$', '$\\eta/s(B)$', '$\\eta/s(C)$', '$\\eta/s(D)$']\n",
    "Xdim = len(ParameterLabels) # dimension of the parameter space\n",
    "Ndesign = 200\n",
    "read_design_from_file = True\n",
    "if read_design_from_file:\n",
    "    design_df = pd.read_csv(data_path('design'))\n",
    "    design = design_df.values\n",
    "    ranges_df = pd.read_csv(data_path('designRange'))\n",
    "    ranges = ranges_df.values\n",
    "    print(f'Load design from file. Design shape is {design.shape}')\n",
    "else:\n",
    "    # range of the design, save to file\n",
    "    ranges = np.array([\n",
    "        [6,15],[.02, 0.2],[-1,4],[0,10],[0.1, 0.4]\n",
    "    ])\n",
    "    ranges_df = pd.DataFrame(ranges, columns=['min','max'], index=ParameterLabels)\n",
    "    ranges_df.to_csv(data_path('designRange'), index=False)\n",
    "    # First make a Latin-Hypercube design within the Xdim unit cube [0,1]^Xdim\n",
    "    np.random.seed(1)\n",
    "    unit = lhs(ranges.shape[0], samples=Ndesign, criterion='maximin')\n",
    "    # Then, rescale the design tot he desired range\n",
    "    design = ranges[:,0] + unit*(ranges[:,1]-ranges[:,0])\n",
    "    # Save to file\n",
    "    design_df = pd.DataFrame(design, columns=ParameterLabels)\n",
    "    design_df.to_csv(data_path('design'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 5 #### Please refer to this number in your questions\n",
    "# Define the min and max boundary of the design\n",
    "design_max = ranges[:,1]\n",
    "design_min = ranges[:,0]\n",
    "# The range of the design is an important reference length scale when we train the emulator \n",
    "design_ptp = design_max - design_min\n",
    "ranges_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 6 #### Please refer to this number in your questions\n",
    "design_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 7 #### Please refer to this number in your questions\n",
    "i,j = 1,2\n",
    "hist_1d_2d(design[:,i], design[:,j], ParameterLabels[i], ParameterLabels[j])\n",
    "save_fig(\"Check_design\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: compute model at each design point\n",
    "\n",
    "This simple model only take seconds to finish. However, this is usually the most time-consuming part of the analysis for realistic bulk model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 8 #### Please refer to this number in your questions\n",
    "#Simulation outputs at the design points\n",
    "simulation = np.array([np.concatenate(ToyModel(param)) for param in design])\n",
    "Ydim = simulation.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Emulators for the model\n",
    "\n",
    "### 3.1: dimensional reduction\n",
    "We will use a technique called \"Principal component analysis\" (PCA) to find the few most dominate features in the training data. As a result, insteading of using 10 values of $dN_{\\rm ch}/d\\eta$ and $v_2$ to represent the model at a parameter point, we can use a few ($n\\ll 10$) coefficients of the features of represent the model, i.e., bears the name \"dimensional reduction\".\n",
    "\n",
    "Here, we will not go into the full details of PCA. But a requirement for PCA to be efficiently applied, the data better follows a multi-variate normal distributions when you randomly sample the parameter point. Otherwise, one need to think of good ways to transform the data before apply the PCA. This will get clear in the following exercise.\n",
    "\n",
    "1. Make a scatter plot where  $X=dN_{\\rm ch}/d\\eta[0-5\\%]$ and $Y=v_2[0-5\\%]$ from all the deisgn points. Does the joint distribution looks like a 2D-normal distribution to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 9 #### Please refer to this number in your questions\n",
    "Nc = len(cen)\n",
    "X = simulation[:,0]\n",
    "Y = simulation[:, Nc]\n",
    "hist_1d_2d(X, Y, r\"$dN_{\\rm ch}/d\\eta[0-5\\%]$\", r\"$v_2[0-5\\%]$\")\n",
    "save_fig(\"Check_design_obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The above data clearly does not resembles a 2D normal distribution. But often a simple transformation works. Now, plot $\\sqrt{dN_{\\rm ch}/d\\eta[0-5\\%]}$ v.s. $\\sqrt{v_2[0-5\\%]}$ and check the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 10 #### Please refer to this number in your questions\n",
    "hist_1d_2d(np.sqrt(X), np.sqrt(Y), r\"$\\sqrt{dN_{\\rm ch}/d\\eta[0-5\\%]}$\", r\"$\\sqrt{v_2[0-5\\%]}$\")\n",
    "save_fig(\"Check_design_tranformed_obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not perfect, but now the distribution looks more like a normal distribution, which is importance for high-performance PCA. Note that the transofrmation stress out the dense region in the previous plot and reduce the degree of non-linear correlation. From now on, we will build emulators that learns how to map novel sets of parameters to $Y_{\\rm model} = \\left\\{ \\sqrt{dN_{\\rm ch}/d\\eta}, \\sqrt{v_2} \\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 11 #### Please refer to this number in your questions\n",
    "use_NL = True\n",
    "Y_model = np.sqrt(simulation) if use_NL else simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Standardlized the distribuiton $\\tilde{Y} = (Y-\\bar{Y})/\\sigma_Y$ for each observable. This way the PCA algorith does not need to handle extremely large/small numbers or biased distributions that does not centered around zero. This so-called ``StandardScaler'' class in sklearn takes care this transfomation and its inverse transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 12 #### Please refer to this number in your questions\n",
    "#Scaling the data to be zero mean and unit variance for each feature\n",
    "SS  =  StandardScaler(copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Apply the principal component analysis as implemented by sklearn, and we will only keep the first $N_{\\rm pc}$ features (principal components) of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 13 #### Please refer to this number in your questions\n",
    "Npc = 3\n",
    "pca = PCA(copy=True, whiten=True, svd_solver='full')\n",
    "# Keep only the first `npc` principal components\n",
    "pc_tf_data = pca.fit_transform(SS.fit_transform(Y_model)) [:,:Npc]\n",
    "\n",
    "# The transformation matrix from PC to Physical space\n",
    "inverse_tf_matrix = pca.components_ * np.sqrt(pca.explained_variance_[:, np.newaxis]) * SS.scale_ \n",
    "inverse_tf_matrix = inverse_tf_matrix[:Npc,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How do we know 3 features are enought. We can take a look at the ``importance'' of (data variance contained in) each principal components / features. You will find that the first 3 feaures already explain more than 99% of the total data variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 14 #### Please refer to this number in your questions\n",
    "# print the explained raito of variance\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(7,2.6))\n",
    "#importance = pca_analysis.explained_variance_\n",
    "importance = pca.explained_variance_\n",
    "cumulateive_importance = np.cumsum(importance)/np.sum(importance)\n",
    "idx = np.arange(1,1+len(importance))\n",
    "ax1.bar(idx,importance)\n",
    "ax1.set_xlabel(\"PC index\")\n",
    "ax1.set_ylabel(\"Variance\")\n",
    "ax2.bar(idx,cumulateive_importance)\n",
    "ax2.set_xlabel(r\"The first $n$ PC\")\n",
    "ax2.set_ylabel(\"Fraction of total variance\")\n",
    "plt.tight_layout()\n",
    "save_fig(\"PC_importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training Gaussian process emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved emulators exists and overide is prohibited\n"
     ]
    }
   ],
   "source": [
    "#### Block 15 #### Please refer to this number in your questions\n",
    "import time\n",
    "overide = False\n",
    "EMU = \"emulators.dat\"\n",
    "if (os.path.exists(data_path(EMU))) and (overide==False):\n",
    "    print('Saved emulators exists and overide is prohibited')\n",
    "    with open(data_path(EMU),\"rb\") as f:\n",
    "        Emulators=pickle.load(f)\n",
    "else:\n",
    "    Emulators=[]\n",
    "    for i in range(0,Npc):\n",
    "        start_time = time.time()\n",
    "        kernel=1*krnl.RBF(length_scale=design_ptp,\n",
    "                          length_scale_bounds=np.outer(design_ptp, (1e-2, 1e2)))\\\n",
    "               + krnl.WhiteKernel(noise_level=.1, \n",
    "                                  noise_level_bounds=(1e-2, 1e2))\n",
    "        print(\"-----------------\")\n",
    "        print(\"Training PC #\",i+1)\n",
    "        GPR=gpr(kernel=kernel,n_restarts_optimizer=5)\n",
    "        GPR.fit(design, pc_tf_data[:,i].reshape(-1,1))\n",
    "        print('GPR score: {:1.3f}'.format(GPR.score(design,pc_tf_data[:,i])) )\n",
    "        print(\"time: {:1.3f} seconds\".format(time.time() - start_time))\n",
    "        Emulators.append(GPR)\n",
    "\n",
    "if (overide==True) or not (os.path.exists(data_path(EMU))):\n",
    "    with open(data_path(EMU), \"wb\") as f:\n",
    "        pickle.dump(Emulators,f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 16 #### Please refer to this number in your questions\n",
    "\n",
    "# This function takes model parmaeters, \n",
    "# 1. gets GP emulator prediction on principal components\n",
    "# 2. inverse transform of  PCA+standard scaler\n",
    "# 3. inverse transform the sqrt(*) non-linear transformation that we add by hand\n",
    "def predict_observables(model_parameters, diag_std=False):\n",
    "    \"\"\"Predicts the observables for any model parameter value using the trained emulators.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Theta_input : Model parameter values.\n",
    "    Return\n",
    "    ------\n",
    "    Mean value and full error covaraiance matrix of the prediction is returened. \"\"\"\n",
    "    \n",
    "    mean=[]\n",
    "    variance=[]\n",
    "    theta=np.array(model_parameters).flatten()\n",
    "    \n",
    "    if len(theta)!=Xdim:\n",
    "        raise TypeError('The input model_parameters array does not have the right dimensions')\n",
    "    else: \n",
    "        theta=np.array(theta).reshape(1,Xdim)\n",
    "        for i in range(Npc):\n",
    "            mn,std=Emulators[i].predict(theta, return_std=True)\n",
    "            mean.append(mn)\n",
    "            variance.append(std**2)\n",
    "    mean=np.array(mean).reshape(1,-1)\n",
    "    inverse_transformed_mean = mean@inverse_tf_matrix + np.array(SS.mean_).reshape(1,-1)    \n",
    "    variance_matrix = np.diag(np.array(variance).flatten())\n",
    "    inverse_transformed_variance = np.einsum('ik,kl,lj-> ij', inverse_tf_matrix.T, variance_matrix, inverse_tf_matrix, \n",
    "                                             optimize=False)\n",
    "    if use_NL:\n",
    "        inverse_transformed_mean = inverse_transformed_mean**2\n",
    "        inverse_transformed_variance *= np.outer(2.*inverse_transformed_mean[0]**.5, \n",
    "                                                 2.*inverse_transformed_mean[0]**.5)\n",
    "    if diag_std:\n",
    "        return inverse_transformed_mean[0], np.sqrt(np.diag(inverse_transformed_variance))\n",
    "    else:\n",
    "        return inverse_transformed_mean[0], inverse_transformed_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Emulator validation\n",
    "\n",
    "Before applying the above model predictor based on PCA+GP, it is impertaive to validate its performance.\n",
    "The most straight forward way is to \n",
    "\n",
    "1. Sample novel parameters sets other than those used to train GPs. \n",
    "2. Compare GP prediction to the physical model calculations at these novel parameter points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Block 17 #### Please refer to this number in your questions\n",
    "# randomly sample another 2 parametr points from the parameter space\n",
    "np.random.seed(9)\n",
    "X_validation = design_min + np.random.rand(2, Xdim)*design_ptp\n",
    "\n",
    "# Next, get the emulator prediction and uncertainty\n",
    "A = np.array([predict_observables(it, diag_std=True) for it in X_validation])\n",
    "Y_predicted = A[:,0,:]\n",
    "Y_std = A[:,1,:]\n",
    "\n",
    "# Model calculation at these two points\n",
    "Y_validation = np.array([np.concatenate(ToyModel(param)) for param in X_validation])\n",
    "\n",
    "# plot the prediction + uncertainty band with the true model caluclation\n",
    "fig, axes = plt.subplots(1,2, figsize=(7,3.5), sharex=True)\n",
    "for i, (mean, std) in enumerate(zip(Y_predicted, Y_std)):\n",
    "    label = 'GP emulated' if i==0 else''\n",
    "    axes[0].fill_between(cen,mean[:Nc]-std[:Nc],mean[:Nc]+std[:Nc],color='r',alpha=.5, label=label)\n",
    "    axes[1].fill_between(cen,mean[Nc:]-std[Nc:],mean[Nc:]+std[Nc:],color='r',alpha=.5, label=label)\n",
    "for i, ym in enumerate(Y_validation):\n",
    "    label = 'Model calc.' if i==0 else''\n",
    "    axes[0].plot(cen, ym[:Nc], 'b.', label=label)\n",
    "    axes[1].plot(cen, ym[Nc:], 'b.', label=label)\n",
    "\n",
    "# Add labels\n",
    "labels = r\"$dN/d\\eta$\", r\"$v_2$\", r\"$\", r\"$dN/d\\eta$: pred-true\", r\"$v_2$: pred-true\"\n",
    "for ax, label in zip(axes, labels):\n",
    "    ax.set_xlabel(\"Centrality(%)\")\n",
    "    ax.set_ylabel(label)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(ymin=0)\n",
    "plt.tight_layout(True)\n",
    "save_fig(\"Emulator_validation_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that emulaotr prediction is not perfect and it estimates its own interpolation uncertainty. How do we know the emulation is consistent with model calcualtion within uncertainty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 18 #### Please refer to this number in your questions\n",
    "\n",
    "# Test more extensively! Sample 100 more validation points\n",
    "X_validation = design_min + np.random.rand(100, Xdim)*design_ptp\n",
    "Y_validation = np.array([np.concatenate(ToyModel(param)) for param in X_validation])\n",
    "\n",
    "A = np.array([predict_observables(it, diag_std=True) for it in X_validation])\n",
    "Y_predicted = A[:,0,:]\n",
    "Y_std = A[:,1,:]\n",
    "\n",
    "# This time, we use a self-normalized quantity to check if the uncertainty estimation is\n",
    "# resaonble: delta = (y_emulated - y_model)/std_emulated\n",
    "normalized_discrepancy = ((Y_predicted - Y_validation)/Y_std).flatten()\n",
    "_ = plt.hist(normalized_discrepancy, bins=31, range=[-5,5], density=True)\n",
    "\n",
    "# It should look like a standard normal distribution\n",
    "x = np.linspace(-3,3,100)\n",
    "from scipy.stats import norm\n",
    "plt.plot(x, norm.pdf(x,0,1))\n",
    "plt.xlabel(r\"$d=\\frac{y_{\\rm emulator}-y_{\\rm truth}}{\\delta y_{\\rm emulator}}$\", fontsize=15)\n",
    "plt.ylabel(r\"P(d)\", fontsize=15)\n",
    "plt.xlim(-5,5)\n",
    "save_fig(\"Emulator_validation_self_normed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Bayesian Inference (we can take a break here)\n",
    "\n",
    "### 4.1: Define experimental dataset, uncertainty, and experimental covariance matrix\n",
    "\n",
    "In this exercises, we use the so-called \"Pseudodata\". Pseudodata uses the model calulations from a specific set of parameters. This set of parameters is the \"true value\" that can be compared with the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 19 #### Please refer to this number in your questions\n",
    "# Pseudo data generated with \"true\" parameters\n",
    "true_values = np.array([9.0, 0.1, 1, 4, 0.18])\n",
    "y_exp = np.concatenate( ToyModel(true_values) )\n",
    "\n",
    "# Put in statistical flucuation in the Pseudodata\n",
    "stat_level = 0.05\n",
    "np.random.seed(5)\n",
    "y_exp *= (1. + np.random.normal(0,stat_level,len(y_exp)))\n",
    "\n",
    "# But we know there is a 5% stat uncertainty\n",
    "y_stat = y_exp * stat_level\n",
    "# this is used to construct the experimetnal uncertainty.\n",
    "y_exp_variance = np.diag(y_stat**2)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(6,3), sharex=True)\n",
    "for ax,y,name in zip(axes,  [y_exp[:Nc], y_exp[Nc:]], [r\"$N_{\\rm ch}$\", r\"$v_2$\"]):\n",
    "    ax.errorbar(cen, y, yerr=y*stat_level, fmt='k.')\n",
    "    ax.set_ylabel(name)\n",
    "    ax.set_xlabel(\"Centrality (%)\")\n",
    "    ax.set_ylim(ymin=0, ymax=y.max()*1.2)\n",
    "plt.tight_layout()\n",
    "save_fig(\"Pseudodata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2: Define prior, likelihood and posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 20 #### Please refer to this number in your questions\n",
    "def log_prior(model_parameters):\n",
    "    \"\"\"Evaluvate the prior at model prameter values. \n",
    "    If all parameters are inside bounds function will return 0 otherwise -inf\"\"\"\n",
    "    X = np.array(model_parameters).reshape(1,-1)\n",
    "    lower = np.all(X >= design_min)\n",
    "    upper = np.all(X <= design_max)\n",
    "    if (lower and upper):\n",
    "        lp=0\n",
    "    # lp = np.log(st.beta.pdf(X,5,1,dsgn_min_ut.reshape(1,-1),(dsgn_max_ut-dsgn_min_ut).reshape(1,-1))).sum()\n",
    "    else:\n",
    "        lp = -np.inf\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 21 #### Please refer to this number in your questions\n",
    "def mvn_loglike(y, cov):\n",
    "    \"\"\"\n",
    "    Evaluate the multivariate-normal log-likelihood for difference vector `y`\n",
    "    and covariance matrix `cov`:\n",
    "\n",
    "        log_p = -1/2*[(y^T).(C^-1).y + log(det(C))] + const.\n",
    "\n",
    "    The likelihood is NOT NORMALIZED, since this does not affect MCMC.  The\n",
    "    normalization const = -n/2*log(2*pi), where n is the dimensionality.\n",
    "\n",
    "    Arguments `y` and `cov` MUST be np.arrays with dtype == float64 and shapes\n",
    "    (n) and (n, n), respectively.  These requirements are NOT CHECKED.\n",
    "\n",
    "    The calculation follows algorithm 2.1 in Rasmussen and Williams (Gaussian\n",
    "    Processes for Machine Learning).\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the Cholesky decomposition of the covariance.\n",
    "    # Use bare LAPACK function to avoid scipy.linalg wrapper overhead.\n",
    "    L, info = lapack.dpotrf(cov, clean=False)\n",
    "\n",
    "    if info < 0:\n",
    "        raise ValueError(\n",
    "            'lapack dpotrf error: '\n",
    "            'the {}-th argument had an illegal value'.format(-info)\n",
    "        )\n",
    "    elif info < 0:\n",
    "        raise np.linalg.LinAlgError(\n",
    "            'lapack dpotrf error: '\n",
    "            'the leading minor of order {} is not positive definite'\n",
    "            .format(info)\n",
    "        )\n",
    "\n",
    "    # Solve for alpha = cov^-1.y using the Cholesky decomp.\n",
    "    alpha, info = lapack.dpotrs(L, y)\n",
    "\n",
    "    if info != 0:\n",
    "        raise ValueError(\n",
    "            'lapack dpotrs error: '\n",
    "            'the {}-th argument had an illegal value'.format(-info)\n",
    "         )\n",
    "\n",
    "    if np.all(L.diagonal()>0):\n",
    "        return -.5*np.dot(y, alpha) - np.log(L.diagonal()).sum()\n",
    "    else:\n",
    "        return -.5*np.dot(y, alpha) - np.log(np.abs(L.diagonal())).sum()\n",
    "        print(L.diagonal())\n",
    "        raise ValueError(\n",
    "            'L has negative values on diagonal {}'.format(L.diagonal())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 22 #### Please refer to this number in your questions\n",
    "# Covariance truncation error from PC is not yet included\n",
    "def log_posterior(model_parameters):\n",
    "    mn, var = predict_observables(model_parameters)\n",
    "    delta_y = mn - y_exp\n",
    "    delta_y = delta_y.flatten()   \n",
    "    total_var = var + y_exp_variance\n",
    "    return log_prior(model_parameters) + mvn_loglike(delta_y,total_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3: Draw random samples according to the posterior distribution using the Markov-chain Monte-Carlo (MCMC) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 23 #### Please refer to this number in your questions\n",
    "do_mcmc = False\n",
    "nwalkers = 10*Xdim  # number of MCMC walkers\n",
    "nburn = 500 # \"burn-in\" period to let chains stabilize\n",
    "nsteps = 2000  # number of MCMC steps to take\n",
    "filename = data_path(name+\".h5\")\n",
    "\n",
    "if do_mcmc==True:\n",
    "    #backend = emcee.backends.HDFBackend(filename)\n",
    "    starting_guesses = design_min + (design_max - design_min) * np.random.rand(nwalkers, Xdim)\n",
    "    #print(starting_guesses)\n",
    "    print(\"MCMC sampling using emcee (affine-invariant ensamble sampler) with {0} walkers\".format(nwalkers))\n",
    "    with Pool() as pool:\n",
    "        sampler = emcee.EnsembleSampler(nwalkers, Xdim, log_posterior)\n",
    "        print('burn in sampling started')    \n",
    "        pos = sampler.run_mcmc(starting_guesses, nburn, progress=True, store=True)\n",
    "        print(\"Mean acceptance fraction: {0:.3f} (in total {1} steps)\".format(\n",
    "                        np.mean(sampler.acceptance_fraction), nwalkers*nburn))\n",
    "        print('Burn in completed.')\n",
    "        print(\"Now running the samples\")\n",
    "        sampler.run_mcmc(initial_state=None, nsteps=nsteps, progress=True, tune=False)  \n",
    "        print(\"Mean acceptance fraction: {0:.3f} (in total {1} steps)\".format(\n",
    "                        np.mean(sampler.acceptance_fraction), nwalkers*nsteps))\n",
    "        \n",
    "        # discard burn-in points and flatten the walkers; the shape of samples is (nwalkers*nsteps, Xdim)\n",
    "        #samples = backend.get_chain(flat=True, discard=nburn)\n",
    "        samples = sampler.get_chain(flat=True, discard=nburn)\n",
    "        samples_df = pd.DataFrame(samples, columns=ParameterLabels)\n",
    "        samples_df.to_csv(data_path('ChainForSImpleModel'), index=False)\n",
    "else:\n",
    "    #df1=h5py.File(filename,mode='r')\n",
    "    print('Loading chain from disk')\n",
    "    samples_df=pd.read_csv(data_path('ChainForSImpleModel'))\n",
    "    samples= samples_df.values\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Explore the posterior distribution\n",
    "\n",
    "The posterior distibution contains all the information, but what would be a good central prediction to quote? \n",
    "\n",
    "Try: \n",
    "1. MAP (Maximum a posterior)\n",
    "2. The median value of each parameter's posterior distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 24 #### Please refer to this number in your questions\n",
    "USE_MAP = False\n",
    "if USE_MAP:\n",
    "    bounds=[(a,b) for (a,b) in zip(design_min,design_max)]\n",
    "    parameters0 = optimize.differential_evolution(lambda x: -log_posterior(x), \n",
    "                                    bounds=bounds,\n",
    "                                    tol=1e-9,\n",
    "                                    ).x\n",
    "else:\n",
    "    parameters0 = [np.percentile(it,50) for it in samples.T]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the posterior distribution (single-parameer, and joint two-parameter distributions) along with the true values and central estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 25 #### Please refer to this number in your questions\n",
    "g = sns.PairGrid(samples_df.iloc[:,:], corner=True, diag_sharey=False)\n",
    "g.map_lower(sns.histplot, bins=100, color=sns.color_palette()[9])\n",
    "g.map_diag(sns.kdeplot, linewidth=2, shade=True, color=sns.color_palette()[-1])\n",
    "for n in range(Xdim):\n",
    "    ax=g.axes[n][n]\n",
    "    ax.axvline(x=parameters0[n], ls='-', c=sns.color_palette()[9], label='Central Estimation')\n",
    "    ax.axvline(x=true_values[n], ls='-', c=sns.color_palette()[3], label='Truth')\n",
    "    ax.text(0,0.9,s= f'{parameters0[n]:.2f}', transform=ax.transAxes, color=sns.color_palette()[9], fontsize=12)\n",
    "    ax.text(0,0.8,s= f'{true_values[n]:.2f}', transform=ax.transAxes, color=sns.color_palette()[3], fontsize=12)\n",
    "g.axes[1,1].legend(loc='best', fontsize=10)\n",
    "for i in range(Xdim):\n",
    "    for j in range(i+1):\n",
    "        g.axes[i,j].set_xlim(*ranges[j])\n",
    "        if i==j:\n",
    "            g.axes[i,j].set_ylim(*ranges[i])\n",
    "            \n",
    "        else:\n",
    "            g.axes[i,j].set_ylim(ymax=0)\n",
    "            g.axes[i,j].axvline(x=parameters0[j], ls='-', c=sns.color_palette()[9])\n",
    "            g.axes[i,j].axvline(x=true_values[j], ls='-', c=sns.color_palette()[3])\n",
    "            g.axes[i,j].axhline(y=parameters0[i], ls='-', c=sns.color_palette()[9])\n",
    "            g.axes[i,j].axhline(y=true_values[i], ls='-', c=sns.color_palette()[3])\n",
    "            g.axes[i,j].scatter(parameters0[j], parameters0[i], color=sns.color_palette()[9])\n",
    "            g.axes[i,j].scatter(true_values[j], true_values[i], color=sns.color_palette()[3])\n",
    "plt.tight_layout()\n",
    "save_fig(\"Posterior_of_parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the viscosity parametrization, we are not interested in any individual parameters. Because, first they are highly-correlated, and second the meanning of each parameter is very specificy to the form of parametrization. It is more reasonable to direclty look at the functional posterior of $\\eta/s(T)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 26 #### Please refer to this number in your questions\n",
    "Tt = np.linspace(0.1, .8, 100)\n",
    "Nsamples = 10000\n",
    "Nthin = samples_df.size//Nsamples\n",
    "# prior samples of eta/s(T)\n",
    "prior_etas = np.array([\n",
    "        etas(Tt, row[1:]) for row in ((ranges[:,1]-ranges[:,0])*np.random.rand(Nsamples,Xdim)+ranges[:,0])\n",
    "])\n",
    "# posterior samples of eta/s(T)\n",
    "posterior_etas = np.array([\n",
    "        etas(Tt, row[1:]) for row in samples_df.iloc[::Nthin,:].values\n",
    "])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6,5), sharex=False, sharey=False, constrained_layout=True)\n",
    "fig.suptitle(\"Posterior v.s. Prior\", wrap=True)\n",
    "# plot prior, display 30%, 60%, 90%, and 100% credible limits\n",
    "CLbins = [30,60,90, 95]\n",
    "for CL, opacity in zip(CLbins, [.35,.3,.25,.2]):\n",
    "    lower, upper = np.percentile(prior_etas, [50-CL/2., 50+CL/2.], axis=0)\n",
    "    ax.fill_between(Tt,lower, upper,color=sns.color_palette()[0], alpha=opacity, label='{:d}% Prior'.format(CL))\n",
    "# plot posterior, display 30%, 60%, and 90% credible limits\n",
    "CLbins = [30,60,90, 95]\n",
    "for CL, opacity in zip(CLbins, [.35,.3,.25,.2]):\n",
    "    lower, upper = np.percentile(posterior_etas, [50-CL/2., 50+CL/2.], axis=0)\n",
    "    ax.fill_between(Tt,lower, upper,color=sns.color_palette()[3], alpha=opacity, label='{:d}% Posterior'.format(CL))\n",
    "        \n",
    "ax.plot(Tt, etas(Tt, true_values[1:]),'k-', lw=3, label='Truth', alpha=.4)\n",
    "ax.plot(Tt, etas(Tt, parameters0[1:]),'--', color=plt.cm.Reds(.5), lw=3, label='Central Estimation')\n",
    "\n",
    "ax.legend(loc='upper left', ncol=2)\n",
    "ax.set_ylim(0,.8)\n",
    "ax.set_xlabel('T [GeV]')\n",
    "ax.set_ylabel('$\\eta/s$')\n",
    "save_fig(\"Posterior_of_eta_s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is always important to check if the observables are well described.\n",
    "It is possible that a poorly performing model produce a tightly-constrained posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 27 #### Please refer to this number in your questions\n",
    "Nsamples = 100\n",
    "Nthin = samples_df.size//Nsamples\n",
    "# prior parameter samples\n",
    "prior_params = (ranges[:,1]-ranges[:,0])*np.random.rand(Nsamples,Xdim) + ranges[:,0]\n",
    "# posterior parameter samples \n",
    "posterior_params =  samples_df.iloc[::Nthin,:].values\n",
    "\n",
    "prior_obs = [np.concatenate(ToyModel(p)) for p in prior_params]\n",
    "posterior_obs = [np.concatenate(ToyModel(p)) for p in posterior_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Block 28 #### Please refer to this number in your questions\n",
    "fig, axes = plt.subplots(2,2, figsize=(6,6), sharex=True)\n",
    "CLbins = [30,60,90, 95]\n",
    "for isample, ilabel, color in zip([prior_obs, posterior_obs], \n",
    "                               ['prior','posterior'], \n",
    "                               [sns.color_palette()[0], sns.color_palette()[3]]):\n",
    "    for CL, opacity in zip(CLbins, [.35, .3, .25, .2]):\n",
    "        label = '{:d}% {}'.format(CL, ilabel)\n",
    "        lower, upper = np.percentile(isample, [50-CL/2., 50+CL/2.], axis=0)\n",
    "        axes[0,0].fill_between(cen, lower[:Nc], upper[:Nc], color=color, alpha=opacity, label=label)\n",
    "        axes[0,1].fill_between(cen, lower[Nc:], upper[Nc:], color=color, alpha=opacity, label=label)\n",
    "        axes[1,0].fill_between(cen, (lower/y_exp)[:Nc], (upper/y_exp)[:Nc], color=color, alpha=opacity, label=label)\n",
    "        axes[1,1].fill_between(cen, (lower/y_exp)[Nc:], (upper/y_exp)[Nc:], color=color, alpha=opacity, label=label)\n",
    "    \n",
    "for ax,y,name in zip(axes[0], \n",
    "                    [y_exp[:Nc], y_exp[Nc:]],\n",
    "                    [r\"$N_{\\rm ch}$\", r\"$v_2$\"]):\n",
    "    ax.errorbar(cen, y, yerr=y*.05, fmt='k.')\n",
    "    ax.set_ylabel(name)\n",
    "    ax.set_xlabel(\"Centrality (%)\")\n",
    "    \n",
    "for ax,y,name in zip(axes[1], \n",
    "                    [y_exp[:Nc], y_exp[Nc:]],\n",
    "                    [\"Ratio to pseudo-data$\"]*2):\n",
    "    ax.errorbar(cen, y/y, yerr=y/y*.05, fmt='k.')\n",
    "    ax.set_ylabel(name)\n",
    "    ax.set_xlabel(\"Centrality (%)\")\n",
    "axes[0,0].semilogy()\n",
    "axes[0,1].semilogy()\n",
    "axes[1,0].set_ylim(0,3)\n",
    "axes[1,1].set_ylim(0,3)\n",
    "plt.tight_layout()\n",
    "save_fig(\"Posterior_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to try:\n",
    "1. Change the $p$ parameter in the defintion of the effective shear viscosity to $p=-3$ and repeat the analysis. Can you still constain the viscosity at high temperature?\n",
    "2. Change the uncertainty level of the pseudodata and repeat.\n",
    "3. Vary the number of PC. Is it always good to include more PC?\n",
    "4. What if we do not include the non-linear transformation, and force Gaussian process to learn something that is clearly not normally distributed? \n",
    "5. What will happen if we use a different $\\eta/s$ parametrization to generate the pseudodata. Or more generally, what can we learn from this piece-wise linearly form of $\\eta/s$, given that nature may not have such form of $\\eta/s(T)$?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
